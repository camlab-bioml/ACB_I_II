---
title: "An introduction to supervised machine learning"
# subtitle: "âš”<br/>with xaringan"
author: "Kieran Campbell"
institute: "Lunenfeld Tanenbaum Research Institute & University of Toronto"
date: "2021-02-22 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, metropolis, tamu-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r, include=FALSE}
suppressPackageStartupMessages({
  library(tidyverse)
})

theme_set(cowplot::theme_cowplot())
```


class: inverse

# What we'll cover

- What is supervised learning?

- Continuous outputs: linear regression

- Categorical outputs: logistic regression

- A gallery of more sophisticated models

- Controlling model complexity


---



# What is supervised learning?

--

Given some input data, we would like to train a model that _accurately_ predicts a corresponding output

--

## Examples

--

1. Given todays's weather (rainfall, hours of sunshine, average windspeed), will it rain tomorrow?
   - **Input**: today's weather, **output**: tomorrow's weather

--

2. Given a patient's laboratory test values (creatinine, glucose), does the patient have a particular disease?
  - **Input**: lab test values, **output**: diagnosis

--

3. Given a cancer biopsy gene expression measurement, what is the tissue of origin?
  - **Input**: gene expression for a given biopsy, **output**: cancer tissue of origin

---

# What's special compared to other prediction types?


--

Supervised machine learning explicitly involves

1. Past data where you _know_ the correct answer (input output pairs)
2. A model that we train on past data to get _good_ at predicting outputs given inputs

--

Contrast this with:

1. **Rule based predictions**: e.g. if `blood glucose > 125mg/dL`, patient has diabetes
2. **Simulation based predictions**: e.g. use physics simulator to forward simulate tomorrow's weather, given today's

--

## When should we use supervised learning?

If you have sufficient, `r icon::fontawesome("star")` 
**unbiased** `r icon::fontawesome("star")` data, it _may_ be better than other approaches

---

class: top

# What's different from supervised?



Unsupervised learning (typically) has no outputs measured<sup>1</sup>
.footnote[
[1] Very common in biomedical applications
]


Instead we "find structure" in the data


## Examples

1. **Netflix**: are there certain types of viewers who prefer particular genres?

2. **Clinical oncology**: are there subtypes of breast cancer that have similar prognosis / respond similarly to treatment?


###  The line between supervised and unsupervised learning is not always clear cut!

---

class: center

# Gene expression subtypes of pancreatic cancer

![PDAC subtypes](intro-ml_figs/pdac-subtypes.png)

---

# The Big Picture

Three ingredients of supervised ML 

1. Some training data (input-output pairs)

--

2. A _model_ that maps **input data** to **output predictions**

--

3. A _loss function_ that measures the discrepancy between the **output prediction** and **output data**

--

we aim to **iteratively improve the model to minimize the loss**.

--

## The Bigger Picture

In probabilistic ML `r icon::fontawesome("arrow-right")` maximize joint probability of the data given the model<sup>1</sup>

.footnote[
[1] Or thereabouts
]

---

class: inverse, center, middle

# The Great Divide

--

Always ask

_is my output continuous or discrete?_



---

# Supervised learning

.pull-left[

## Continuous outputs

* Typically known as _regression_
* Loss is often mean square error
* Example models:
 * Linear regression
 * Polynomial / spline regression
 * Random forests
 * Deep neural networks
 
```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=4,fig.height=3}
df <- tibble(
  Input = runif(100),
  Output = 3 * Input - 1 + rnorm(100, 0, 0.4)
)

ggplot(df, aes(x = Input, y = Output)) +
  geom_point() 
```
]

--

.pull-right[

## Discrete outputs

* Typically known as _classification_
* Loss is often binary cross entropy
* Example models:
 * Logistic regression regression
 * Naive Bayes
 * Support vector machines
 * Deep neural networks

```{r, echo=FALSE, warning=FALSE, message=FALSE, fig.width=4,fig.height=3}
sigmoid <- function(x) {
  1 / (1 + exp(-x))
}

df <- tibble(
  Input = runif(100),
  Output = as.factor(round(sigmoid(5 * (Input-0.5)) + rnorm(100, sd=0.05)))
)

ggplot(df, aes(x = Input, y = Output)) +
  geom_point() +
  scale_y_discrete()
```

]

---

# Supervised ML

Example problem: predict reduction in tumor volume given expression of a gene

## Notation

$N$ samples indexed by $n = 1, \ldots, N$, _e.g. $N$ patients_

--

Input data $x_n \in \mathbb{R}$ for each sample<sup>1</sup>, _e.g. expression of gene_

.footnote[
[1] In general high dimensional, but we can assume low dimensional here
]

--

Output data $y_n \in \mathbb{R}$ for each sample, _e.g. change in tumour volume_

--

A _model_ $f_\theta$ parametrized by $\theta$ that maps the inputs to predicted outputs $t_n$, i.e.

$$t_n = f_\theta(x_n)$$
--

- $t_n$ is the predicted output for sample $n$
- $y_n$ is the actual output for sample $n$

---

# Defining a loss

We aim to adjust $\theta$ to make $t_n$ as _close_ to $y_n$ averaged over $n$

--

Define some _distance_ between $t_n$ and $y_n$ and minimize wrt $\theta$

--

Common choice: mean squared error

$$\mathrm{MSE(\mathbf{t}, \mathbf{y})} = \frac{1}{N} \sum_{n=1}^N (y_n-t_n)^2$$

--


Remember this is still a function of $\theta$:

$$\mathrm{MSE(\mathbf{t}, \mathbf{y})} = \frac{1}{N} \sum_{n=1}^N (y_n-f_\theta(x_n))^2$$


---

# Linear regression

So what is linear regression? 

--

$$t_n = f_\theta(x_n) = wx_n + b$$
--

* Parameters $\theta = \{w,b\}$

--

* $w$ known as the _weight_, $b$ the bias
--

* Loss now becomes

$$\mathrm{MSE(\mathbf{t}, \mathbf{y})} = \frac{1}{N} \sum_{n=1}^N (y_n-wx_n - b)^2$$

--

Iteratively adjust $w$ and $b$ to minimize $\mathrm{MSE(\mathbf{t}, \mathbf{y})}$

---

# Minimizing losses

So how do we minimize $\mathrm{MSE(\mathbf{t}, \mathbf{y})}$? Strategies:

--

1. Randomly guess<sup>1</sup> $(w,b)$ and choose the values that result in lowest $\mathrm{MSE(\mathbf{t}, \mathbf{y})}$
.footnote[
[1] Not recommended
]

--

2. Differentiate $\mathrm{MSE(\mathbf{t}, \mathbf{y})}$ w.r.t $(w,b)$, set to 0, solve for $(w,b)$

--

3. `r icon::fontawesome("star")` Numerical methods `r icon::fontawesome("star")`
 * Giant scientific field
 * Introduce _gradient descent_ here `r icon::fontawesome("arrow-right")` good in high dimensional problems where gradients of the loss w.r.t. the parameters are available

---

background-image: url('intro-ml_figs/mountain.jpg')
background-position: center
background-size: contain
class: inverse


# Gradient descent


You're at the top of a mountain, it's getting dark, and you need to get down
--
 
* Your position $(x,y)$ is your parameter space $(w,b)$ to explore

--
 
* Your height is $\mathrm{MSE(\mathbf{t}, \mathbf{y})}$ you want to minimize

--

## Q: What's the strategy?

--

Take successive little steps downhill until things flatten out

--

## Local optimality

Note this doesn't guarantee you to get to the _bottom_, only to a much flatter region

 `r icon::fontawesome("arrow-right")` Big problem depending on shape of your mountain / loss function

---

# What is downhill?

> successive little steps downhill

--

.pull-left[
Consider $y=(x-1)^2$, $\frac{\mathrm{d}y}{\mathrm{d}x} = 2(x-1)$
]

.pull-right[
```{r, echo=FALSE, fig.align='center', fig.width=4,fig.height=4}
ggplot() + xlim(-3, 5) + 
  geom_function(fun = function(x) (x-1)^2) + 
  geom_vline(xintercept = 1, color='grey50', linetype=2) +
  labs(x="x", y="y")
```
]

--

Notice:
* When $x > 1$ we want to go to the _left_ and $\frac{\mathrm{d}y}{\mathrm{d}x} > 0$
* When $x < 1$ we want to go to the _right_ and  $\frac{\mathrm{d}y}{\mathrm{d}x} < 0$

The sign of the gradient always points uphill!

---

# Gradient descent

This suggests an iterative scheme:

1. Initialize some values for $(w,b)$

--

2. For a given number of steps:
 * Update $w \leftarrow w - \epsilon \frac{\partial}{\partial w} \mathrm{MSE(\mathbf{t}, \mathbf{y}; w, b)}$
 * Update $b \leftarrow b - \epsilon \frac{\partial}{\partial b} \mathrm{MSE(\mathbf{t}, \mathbf{y}; w, b)}$

--

3. Monitor $MSE(\mathbf{t}, \mathbf{y}; w, b)$ - if it "levels off" we can end with the optimal values $(w,b)$

--

## Learning rate

$\epsilon>0$ is known as the _learning rate_ or _step size_. 

* Important parameter to tune: too large and you overshoot, too small and it's inefficient

---

---
